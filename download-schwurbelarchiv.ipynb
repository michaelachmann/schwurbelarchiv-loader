{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb543bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting internetarchive\n",
      "  Downloading internetarchive-3.5.0.tar.gz (102 kB)\n",
      "     |████████████████████████████████| 102 kB 10.7 MB/s           \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing wheel metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.26.0 in /opt/tljh/user/lib/python3.9/site-packages (from internetarchive) (1.26.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /opt/tljh/user/lib/python3.9/site-packages (from internetarchive) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from internetarchive) (4.62.3)\n",
      "Collecting schema>=0.4.0\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
      "Collecting jsonpatch>=0.4\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting docopt<0.7.0,>=0.6.0\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter-michael/.local/lib/python3.9/site-packages (from requests<3.0.0,>=2.25.0->internetarchive) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/tljh/user/lib/python3.9/site-packages (from requests<3.0.0,>=2.25.0->internetarchive) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests<3.0.0,>=2.25.0->internetarchive) (3.1)\n",
      "Collecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Building wheels for collected packages: internetarchive, docopt\n",
      "  Building wheel for internetarchive (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for internetarchive: filename=internetarchive-3.5.0-py3-none-any.whl size=95916 sha256=8679228476f7ba92bec48ffe6646606973f2775cf5f191cc28bba398b1d1e893\n",
      "  Stored in directory: /home/jupyter-michael/.cache/pip/wheels/fa/13/e1/af00275618c05b9012e4afa5df9d38b8537a321a96ee010a64\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=8764daf6385ee3c1a2a6db3c1d04b997d47671b3fb82bc5e75329379c4832c7b\n",
      "  Stored in directory: /home/jupyter-michael/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built internetarchive docopt\n",
      "Installing collected packages: jsonpointer, contextlib2, schema, jsonpatch, docopt, internetarchive\n",
      "\u001b[33m  WARNING: The script ia is installed in '/home/jupyter-michael/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed contextlib2-21.6.0 docopt-0.6.2 internetarchive-3.5.0 jsonpatch-1.33 jsonpointer-2.4 schema-0.7.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install internetarchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211a2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import internetarchive as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1f9277",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_links = []\n",
    "\n",
    "# Identifier of the Archive.org collection\n",
    "collection_identifier = \"schwurbel-archiv\"\n",
    "\n",
    "# Search for items in the collection\n",
    "items = ia.search_items('collection:' + collection_identifier)\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate over the items\n",
    "for item in items:\n",
    "    data.append({\n",
    "        \"ID\": uuid4(),\n",
    "        \"identifier\": item['identifier'],\n",
    "        \"status\": \"Added\",\n",
    "        \"downloaded\": \"\",\n",
    "        \"group_name\": \"\",\n",
    "        \"crawl_data\": \"\"\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f29dccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.to_csv('2023-07-06-Schwurbelarchiv-Liste.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83eba7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(index, status):\n",
    "    df.at[index, 'status'] = status\n",
    "    df.to_csv('2023-07-06-Schwurbelarchiv-Liste.csv', index=False)  # Save the DataFrame to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e141e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_content(filepath):\n",
    "  # Read the HTML file\n",
    "  with open(filepath, 'r', encoding='utf-8') as file:\n",
    "      html_content = file.read()\n",
    "\n",
    "  # Create a Beautiful Soup object\n",
    "  soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "  messages = soup.find_all('div', class_='message')\n",
    "  data = []\n",
    "\n",
    "  # Iterate over the messages\n",
    "  for message_div in messages:\n",
    "      from_name_div = message_div.find('div', class_='from_name')\n",
    "      if not from_name_div:\n",
    "          #print(message_div)\n",
    "          continue\n",
    "\n",
    "      from_name = from_name_div.get_text(strip=True)\n",
    "      forwarded_div = message_div.find('div', class_='forwarded body')\n",
    "\n",
    "      forwarded_from_name = \"\"\n",
    "      forwarded_message_text = \"\"\n",
    "      forwarded_message_photo = \"\"\n",
    "      forwarded_video_duration = \"\"\n",
    "      if forwarded_div:\n",
    "          forwarded_from_name_div = forwarded_div.find('div', class_='from_name')\n",
    "          forwarded_from_name = forwarded_from_name_div.get_text(strip=True).replace(forwarded_from_name_div.find('span', class_='details').get_text(strip=True), '')\n",
    "\n",
    "          message_text_div = forwarded_div.find('div', class_='text')\n",
    "          forwarded_message_text = ''.join(str(child) for child in message_text_div.contents) if message_text_div else \"\"\n",
    "\n",
    "          message_photo = forwarded_div.find('img')\n",
    "          forwarded_message_photo = message_photo['src'] if message_photo else \"\"\n",
    "\n",
    "          video_duration_div = forwarded_div.find('div', class_='video_duration')\n",
    "          forwarded_video_duration = video_duration_div.get_text(strip=True) if video_duration_div else \"\"\n",
    "\n",
    "\n",
    "      message_text_div = message_div.find('div', class_='text')\n",
    "\n",
    "      if message_text_div:\n",
    "          message_text = ''.join(str(child) for child in message_text_div.contents) if message_text_div else \"\"\n",
    "\n",
    "          message_photo = message_div.find('img')\n",
    "          message_photo = message_photo['src'] if message_photo else \"\"\n",
    "\n",
    "          video_duration_div = message_div.find('div', class_='video_duration')\n",
    "          video_duration = video_duration_div.get_text(strip=True) if video_duration_div else \"\"\n",
    "\n",
    "      date_element = message_div.find('div', class_='pull_right date details')\n",
    "      datetime_value = date_element['title']\n",
    "      datetime_object = datetime.strptime(datetime_value, '%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "      # Append the extracted data to the list\n",
    "      data.append({\n",
    "          'ID': uuid4(),\n",
    "          'From Name': from_name,\n",
    "          'Datetime': datetime_object,\n",
    "          'Message Text': message_text,\n",
    "          'Photo': message_photo,\n",
    "          'Video Duration': video_duration,\n",
    "          'Forwarded From Name': forwarded_from_name,\n",
    "          'Forwarded Message Text': forwarded_message_text,\n",
    "          'Forwarded Photo': forwarded_message_photo,\n",
    "          'Forwarded Video Duration': forwarded_video_duration\n",
    "      })\n",
    "\n",
    "  # Create a pandas DataFrame from the extracted data\n",
    "  return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd2c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3be11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on schwurbelarchiv-06hyV5GgkR -- Siegfried Daebritz\n",
      "Downloading export-part1.zip\n"
     ]
    }
   ],
   "source": [
    "temp_folder_path = \"tmp\"\n",
    "\n",
    "for index, row in df[11:20].iterrows():\n",
    "    identifier = row['identifier']\n",
    "    status = row['status']\n",
    "    if status != 'Done':\n",
    "        # Remove old files        \n",
    "        if os.path.exists(\"export.zip\"):\n",
    "            os.remove(\"export*.zip\")\n",
    "\n",
    "        update_status(index, 'Downloading')\n",
    "\n",
    "        item = ia.get_item(identifier)\n",
    "\n",
    "        # Get group name\n",
    "        name = item.metadata['title']\n",
    "\n",
    "        # Extract the name\n",
    "        name_pattern = r'„(.*?)“'\n",
    "        name_match = re.search(name_pattern, name)\n",
    "        group_name = name_match.group(1) if name_match else \"\"\n",
    "        df.at[index, 'group_name'] = group_name\n",
    "\n",
    "        # Extract the date\n",
    "        date_pattern = r'vom (\\d{2}\\.\\d{2}\\.\\d{4})'\n",
    "        date_match = re.search(date_pattern, name)\n",
    "        crawl_date = date_match.group(1) if date_match else \"\"\n",
    "        # Convert the crawl_date_str to a datetime object\n",
    "        crawl_date = datetime.strptime(crawl_date, \"%d.%m.%Y\")\n",
    "\n",
    "        # Update the 'crawl_date' column in the DataFrame\n",
    "        df.at[index, 'crawl_date'] = crawl_date\n",
    "\n",
    "        print(f\"Working on {identifier} -- {group_name}\")\n",
    "\n",
    "        download_files = []\n",
    "        for f in ia.get_files(identifier):          \n",
    "          if f.format == \"ZIP\":\n",
    "            download_files.append(f)\n",
    "            print(f\"Downloading {f.name}\")\n",
    "            f.download()\n",
    "        \n",
    "\n",
    "        filename = \"\"\n",
    "        if len(download_files) == 1:\n",
    "          filename = \"export.zip\"\n",
    "\n",
    "        else:\n",
    "          filename = \"export-part1.zip\"\n",
    "\n",
    "        update_status(index, 'Extracting')\n",
    "\n",
    "        print(\"Downloaded. Extracting Now.\")\n",
    "\n",
    "        # Extract the export.zip into the temp folder\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "          zip_ref.extractall(temp_folder_path)\n",
    "\n",
    "        update_status(index, 'Scraping')\n",
    "        scraped_df = scrape_content(f\"{temp_folder_path}/messages.html\")\n",
    "        scraped_df.to_csv(f\"data/2023-07-Telegram-{row['ID']}.csv\")\n",
    "\n",
    "        # Remove the temp folder if it exists\n",
    "        if os.path.exists(temp_folder_path):\n",
    "          shutil.rmtree(temp_folder_path)\n",
    "\n",
    "        if len(download_files) > 1:\n",
    "          if os.path.exists(identifier):\n",
    "            shutil.rmtree(identifier)\n",
    "\n",
    "        update_status(index, 'Done')\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
